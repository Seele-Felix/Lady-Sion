---
description: 
globs: 
alwaysApply: false
---
<tdd_philosophy_workflow>
  <core_philosophy>
    <fundamental_belief>
      测试不是为了验证代码，而是为了验证哲学。
      每个测试用例都在问：这个数学定义在实际世界中意味着什么？
      先哲学理念，后技术实现；先验证场景，后代码编写。
    </fundamental_belief>

    <architecture_understanding>
      <!-- AnimaWeave架构的正确理解 -->
      <testing_target>
        我们测试的是：basic插件在图执行中的表现
        不是测试：anima文件解析、plugin内部实现
        接口：awakening(sanctum_path, weave_filename) -> FateEcho
        测试文件：weave图文件，不是anima定义文件
      </testing_target>

      <integration_focus>
        验证的是：数学定义在图执行中的完整表现
        不是语法解析，而是集成能力
        不是单元测试，而是端到端验证
        焦点：哲学理念在实际计算中的体现
      </integration_focus>
    </architecture_understanding>
  </core_philosophy>

  <mandatory_workflow>
    <step_1_philosophical_analysis>
      <!-- 强制第一步：哲学理念分析 -->
      <requirements>
        - 读取相关哲学文档章节
        - 理解要验证的核心理念
        - 引用具体的数学定义
        - 问："这个理念在现实中如何体现？"
      </requirements>
      
      <checkpoint>
        "这个数学定义背后体现了什么哲学理念？"
        "𝒯 = {Int, Bool, String, ...} 在实际世界中意味着什么？"
        在开始任何实现之前，必须引用具体的数学定义
      </checkpoint>
    </step_1_philosophical_analysis>

    <step_2_scenario_design>
      <!-- 强制第二步：验证场景设计 -->
      <requirements>
        - 设计能体现哲学理念的实际业务场景
        - 场景必须有意义，不是玩具例子
        - 反推需要的插件能力（类型、节点、行为）
        - 问："用basic.anima的元素能构造什么有意义的例子？"
      </requirements>

      <checkpoint>
        "要验证这个哲学理念，我们需要什么实际场景？"
        "这个哲学理念在图执行中应该如何体现？"
        "什么样的场景能证明这个概念真的成立？"
      </checkpoint>
    </step_2_scenario_design>

    <step_3_capability_assessment>
      <!-- 强制第三步：基础能力评估 -->
      <requirements>
        - 检查当前basic.anima是否支持验证场景
        - 补充缺失的类型和节点定义
        - 创建描述场景的weave测试图文件
        - 问："这些定义能支撑我们想验证的数学概念吗？"
      </requirements>

      <checkpoint>
        "这些验证场景需要basic插件具备什么能力？"
        "需要什么类型？什么节点？什么行为？"
        "当前的basic.anima和plugin.rs支持这些能力吗？"
      </checkpoint>
    </step_3_capability_assessment>

    <step_4_test_documentation>
      <!-- 强制第四步：测试文档编写 -->
      <requirements>
        - 写测试用例文档，不是代码
        - 明确对应的哲学理念和数学定义
        - 描述预期的验证场景和结果
        - 问："我如何用文字描述这个测试场景？"
      </requirements>

      <checkpoint>
        测试用例文档必须包含：
        - 对应的哲学文档章节
        - 验证的数学定义
        - 使用的basic.anima元素
        - 预期的业务场景
      </checkpoint>
    </step_4_test_documentation>

    <step_5_test_implementation>
      <!-- 强制第五步：测试代码实现 -->
      <requirements>
        - 根据文档机械地写测试代码
        - 使用awakening接口执行weave文件
        - 验证输出，不过度设计
        - 问："这个实现忠实转换了测试文档吗？"
      </requirements>

      <checkpoint>
        根据测试用例文档，机械地写测试代码
        使用awakening接口执行weave文件，验证输出
        这里没有创造性，只有忠实转换
      </checkpoint>
    </step_5_test_implementation>

    <step_6_tdd_iteration>
      <!-- 核心环节：TDD迭代修复 -->
      <requirements>
        - 运行测试，观察失败
        - 分析失败原因，找到缺失的功能层
        - 实现最小功能让测试通过
        - 重复直到测试通过
        - 问："这个失败指向什么缺失的功能？"
      </requirements>

      <iteration_pattern>
        每次失败都指向一个具体的缺失功能层：
        插件能力 → 图执行 → 连接解析 → 字符串处理
        "测试失败说明插件缺少什么功能？"
        只实现够让测试通过的最小功能
        不过度设计，专注于数学定义的验证
      </iteration_pattern>
    </step_6_tdd_iteration>

    <step_7_philosophical_verification>
      <!-- 强制最后步：哲学验证确认 -->
      <requirements>
        - 确认实现体现了原始哲学理念
        - 代码结构反映数学定义的美
        - 真正验证了目标概念
        - 问："我们真的验证了哲学理念吗？"
      </requirements>

      <checkpoint>
        "这个实现体现了原始的哲学理念吗？"
        "代码的结构反映了数学定义的美吗？"
        "我们真的验证了目标概念吗？"
      </checkpoint>
    </step_7_philosophical_verification>
  </mandatory_workflow>

  <prohibited_behaviors>
    <!-- 严格禁止的行为模式 -->
    <no_code_first>
      禁止直接编写代码实现，跳过哲学分析
      禁止测试anima文件解析而非图执行
      禁止过度设计，实现超出测试需求的功能
      禁止忽略TDD失败信息，盲目修复
    </no_code_first>

    <no_wrong_testing_target>
      awakening只能传入weave文件，不是anima文件
      测试的是basic插件在图中的表现，不是内部实现
      测试图执行而不是语法解析
    </no_wrong_testing_target>
  </prohibited_behaviors>

  <error_self_correction>
    <!-- 自我纠正机制 -->
    <code_first_detection>
      如果我直接给出代码实现，立即停止并问：
      "我刚才跳过了哪个思考步骤？"
      "这个需求对应的数学定义是什么？"
      "我有没有先分析验证场景和插件能力需求？"
    </code_first_detection>

    <wrong_test_target_detection>
      如果我直接测试anima文件解析，立即停止并问：
      "我是不是又在测试anima解析而不是图执行？"
      "awakening只能传入weave文件，我是不是搞错了测试对象？"
      "我应该测试的是basic插件在图中的表现，不是内部实现对吗？"
    </wrong_test_target_detection>

    <overdesign_detection>
      如果我给出了太复杂的实现，问：
      "这真的是最小实现吗？"
      "我是不是过度设计了？"
    </overdesign_detection>

    <workflow_reset>
      如果我忘记了正确的TDD流程，强制重新开始：
      "等等，让我重新捋顺逻辑：数学定义→验证场景→插件能力→anima定义→weave图→测试文档→测试代码→插件实现"
    </workflow_reset>
  </error_self_correction>

  <progress_tracking>
    <!-- 进度跟踪系统 -->
    <test_case_states>
      每个测试用例的状态标识：
      📝 设计中 → 🔨 实现中 → ✅ 完成 → 🎯 集成测试
    </test_case_states>

    <documentation_as_progress>
      测试用例文档就是进度条，是沟通工具，不是代码
      测试用例文档必须包含：
      - 对应的哲学文档章节
      - 验证的数学定义
      - 使用的basic.anima元素
      - 预期的业务场景
    </documentation_as_progress>

    <milestone_tracking>
      T1.1 ✅ 完成 - 类型系统基础验证
      T1.2 📝 设计中 - 类型安全验证
      T1.3 📝 设计中 - 类型推导验证
      T1.4 📝 设计中 - 类型组合验证
    </milestone_tracking>
  </progress_tracking>
</tdd_philosophy_workflow>

<activation_conditions>
  <when_to_activate>
    用户提到测试用例或TDD相关需求
    开始实现新的功能或验证数学定义
    重新评估技术状况或架构设计
    需要验证哲学理念在代码中的体现
  </when_to_activate>

  <activation_triggers>
    "写测试"、"验证"、"TDD"等关键词
    提到数学定义或哲学概念
    需要检验某个理念是否成立
    讨论AnimaWeave的核心功能实现
  </activation_triggers>
</activation_conditions>

<success_indicators>
  <philosophical_alignment>
    每个测试都对应明确的哲学理念
    数学定义在代码中得到真实体现
    技术实现反映出概念的本质美
  </philosophical_alignment>

  <technical_robustness>
    测试驱动的迭代发现了系统架构缺陷
    每次失败都指向了具体的功能缺失
    最终实现满足了哲学验证的要求
  </technical_robustness>

  <learning_outcomes>
    通过TDD加深了对系统架构的理解
    验证了哲学理念的实际可行性
    建立了可重复的开发工作流
  </learning_outcomes>
</success_indicators>
